{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# transform the image\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# download the image\n",
    "cifar_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                           download=True, transform=transform)\n",
    "cifar_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                          transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Split: train\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "print(cifar_train)\n",
    "print(cifar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loader and test loader\n",
    "trainloader = torch.utils.data.DataLoader(cifar_train, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(cifar_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reference: using LeNet deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    # init\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # Conv2d1 and 2\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # \n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # the final out put is 10, so the last ouput is 10\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    # forward \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        # full connect layer\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "net = LeNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim is SGD\n",
    "import torch.optim as optim\n",
    "\n",
    "# CrossEntropyLoss loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "Done Training!\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Training...\")\n",
    "for epoch in range(60):\n",
    "    # use loss100 to store 100 batch average loss\n",
    "    loss100 = 0.0\n",
    "    # \n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device) # 注意需要复制到GPU\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        # print(outputs.shape)\n",
    "        # print(labels)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss100 += loss.item()\n",
    "        break\n",
    "        if i % 100 == 99:\n",
    "            print('[Epoch %d, Batch %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, loss100 / 100))\n",
    "            loss100 = 0.0\n",
    "\n",
    "print(\"Done Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        # conv layers: (in_channel size, out_channels size, kernel_size, stride, padding)\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        # max pooling (kernel_size, stride)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # fully conected layers:\n",
    "        self.fc6 = nn.Linear(16384, 4096)\n",
    "        self.fc7 = nn.Linear(4096, 4096)\n",
    "        self.fc8 = nn.Linear(4096, 1000)\n",
    "        self.fc9 = nn.Linear(1000, 100)\n",
    "        self.fc10 = nn.Linear(100, 10)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = F.relu(self.conv3_2(x))\n",
    "        x = F.relu(self.conv3_3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv4_1(x))\n",
    "        x = F.relu(self.conv4_2(x))\n",
    "        x = F.relu(self.conv4_3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv5_1(x))\n",
    "        x = F.relu(self.conv5_2(x))\n",
    "        x = F.relu(self.conv5_3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 16384)\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc8(x)\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc9(x)\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc10(x)\n",
    "\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class newNet(nn.Module):\n",
    "    # init\n",
    "    def __init__(self):\n",
    "        super(newNet, self).__init__()\n",
    "        # Conv2d1 and 2\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.bn1=nn.BatchNorm2d(6)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.bn2=nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 5)\n",
    "        self.bn3=nn.BatchNorm2d(32)\n",
    "\n",
    "        # \n",
    "        self.fc1 = nn.Linear(1024, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # the final out put is 10, so the last ouput is 10\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.elu = nn.ELU()\n",
    "    # forward \n",
    "    def forward(self, x):\n",
    "        x = self.elu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, 0.2)\n",
    "        \n",
    "        x = self.elu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, 0.2)\n",
    "        \n",
    "        x = self.elu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, 0.2)\n",
    "        \n",
    "\n",
    "        # full connect layer\n",
    "        x = x.view(-1, 1024)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "net = newNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(cifar_train, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(cifar_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VGG16' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-92e06448b98f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'VGG16' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "net = VGG16().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropyLoss is loss function\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "[Epoch 1, Batch   100] loss: 2.303\n",
      "[Epoch 1, Batch   200] loss: 2.303\n",
      "[Epoch 1, Batch   300] loss: 2.301\n",
      "[Epoch 1, Batch   400] loss: 2.297\n",
      "[Epoch 1, Batch   500] loss: 2.293\n",
      "[Epoch 1, Batch   600] loss: 2.285\n",
      "[Epoch 1, Batch   700] loss: 2.265\n",
      "[Epoch 1, Batch   800] loss: 2.237\n",
      "[Epoch 1, Batch   900] loss: 2.221\n",
      "[Epoch 1, Batch  1000] loss: 2.192\n",
      "[Epoch 1, Batch  1100] loss: 2.163\n",
      "[Epoch 1, Batch  1200] loss: 2.130\n",
      "[Epoch 1, Batch  1300] loss: 2.100\n",
      "[Epoch 1, Batch  1400] loss: 2.097\n",
      "[Epoch 1, Batch  1500] loss: 2.063\n",
      "[Epoch 2, Batch   100] loss: 2.053\n",
      "[Epoch 2, Batch   200] loss: 2.013\n",
      "[Epoch 2, Batch   300] loss: 2.005\n",
      "[Epoch 2, Batch   400] loss: 2.001\n",
      "[Epoch 2, Batch   500] loss: 1.975\n",
      "[Epoch 2, Batch   600] loss: 1.973\n",
      "[Epoch 2, Batch   700] loss: 1.947\n",
      "[Epoch 2, Batch   800] loss: 1.941\n",
      "[Epoch 2, Batch   900] loss: 1.939\n",
      "[Epoch 2, Batch  1000] loss: 1.896\n",
      "[Epoch 2, Batch  1100] loss: 1.896\n",
      "[Epoch 2, Batch  1200] loss: 1.875\n",
      "[Epoch 2, Batch  1300] loss: 1.851\n",
      "[Epoch 2, Batch  1400] loss: 1.847\n",
      "[Epoch 2, Batch  1500] loss: 1.826\n",
      "[Epoch 3, Batch   100] loss: 1.797\n",
      "[Epoch 3, Batch   200] loss: 1.763\n",
      "[Epoch 3, Batch   300] loss: 1.773\n",
      "[Epoch 3, Batch   400] loss: 1.735\n",
      "[Epoch 3, Batch   500] loss: 1.707\n",
      "[Epoch 3, Batch   600] loss: 1.706\n",
      "[Epoch 3, Batch   700] loss: 1.692\n",
      "[Epoch 3, Batch   800] loss: 1.668\n",
      "[Epoch 3, Batch   900] loss: 1.632\n",
      "[Epoch 3, Batch  1000] loss: 1.657\n",
      "[Epoch 3, Batch  1100] loss: 1.621\n",
      "[Epoch 3, Batch  1200] loss: 1.612\n",
      "[Epoch 3, Batch  1300] loss: 1.566\n",
      "[Epoch 3, Batch  1400] loss: 1.579\n",
      "[Epoch 3, Batch  1500] loss: 1.550\n",
      "[Epoch 4, Batch   100] loss: 1.558\n",
      "[Epoch 4, Batch   200] loss: 1.517\n",
      "[Epoch 4, Batch   300] loss: 1.492\n",
      "[Epoch 4, Batch   400] loss: 1.498\n",
      "[Epoch 4, Batch   500] loss: 1.529\n",
      "[Epoch 4, Batch   600] loss: 1.503\n",
      "[Epoch 4, Batch   700] loss: 1.482\n",
      "[Epoch 4, Batch   800] loss: 1.469\n",
      "[Epoch 4, Batch   900] loss: 1.469\n",
      "[Epoch 4, Batch  1000] loss: 1.472\n",
      "[Epoch 4, Batch  1100] loss: 1.464\n",
      "[Epoch 4, Batch  1200] loss: 1.462\n",
      "[Epoch 4, Batch  1300] loss: 1.466\n",
      "[Epoch 4, Batch  1400] loss: 1.463\n",
      "[Epoch 4, Batch  1500] loss: 1.438\n",
      "[Epoch 5, Batch   100] loss: 1.441\n",
      "[Epoch 5, Batch   200] loss: 1.411\n",
      "[Epoch 5, Batch   300] loss: 1.401\n",
      "[Epoch 5, Batch   400] loss: 1.399\n",
      "[Epoch 5, Batch   500] loss: 1.390\n",
      "[Epoch 5, Batch   600] loss: 1.410\n",
      "[Epoch 5, Batch   700] loss: 1.406\n",
      "[Epoch 5, Batch   800] loss: 1.394\n",
      "[Epoch 5, Batch   900] loss: 1.405\n",
      "[Epoch 5, Batch  1000] loss: 1.407\n",
      "[Epoch 5, Batch  1100] loss: 1.416\n",
      "[Epoch 5, Batch  1200] loss: 1.356\n",
      "[Epoch 5, Batch  1300] loss: 1.390\n",
      "[Epoch 5, Batch  1400] loss: 1.376\n",
      "[Epoch 5, Batch  1500] loss: 1.400\n",
      "[Epoch 6, Batch   100] loss: 1.332\n",
      "[Epoch 6, Batch   200] loss: 1.400\n",
      "[Epoch 6, Batch   300] loss: 1.375\n",
      "[Epoch 6, Batch   400] loss: 1.363\n",
      "[Epoch 6, Batch   500] loss: 1.349\n",
      "[Epoch 6, Batch   600] loss: 1.307\n",
      "[Epoch 6, Batch   700] loss: 1.343\n",
      "[Epoch 6, Batch   800] loss: 1.350\n",
      "[Epoch 6, Batch   900] loss: 1.305\n",
      "[Epoch 6, Batch  1000] loss: 1.314\n",
      "[Epoch 6, Batch  1100] loss: 1.311\n",
      "[Epoch 6, Batch  1200] loss: 1.324\n",
      "[Epoch 6, Batch  1300] loss: 1.305\n",
      "[Epoch 6, Batch  1400] loss: 1.289\n",
      "[Epoch 6, Batch  1500] loss: 1.309\n",
      "[Epoch 7, Batch   100] loss: 1.288\n",
      "[Epoch 7, Batch   200] loss: 1.288\n",
      "[Epoch 7, Batch   300] loss: 1.283\n",
      "[Epoch 7, Batch   400] loss: 1.306\n",
      "[Epoch 7, Batch   500] loss: 1.295\n",
      "[Epoch 7, Batch   600] loss: 1.259\n",
      "[Epoch 7, Batch   700] loss: 1.256\n",
      "[Epoch 7, Batch   800] loss: 1.276\n",
      "[Epoch 7, Batch   900] loss: 1.294\n",
      "[Epoch 7, Batch  1000] loss: 1.284\n",
      "[Epoch 7, Batch  1100] loss: 1.231\n",
      "[Epoch 7, Batch  1200] loss: 1.293\n",
      "[Epoch 7, Batch  1300] loss: 1.257\n",
      "[Epoch 7, Batch  1400] loss: 1.266\n",
      "[Epoch 7, Batch  1500] loss: 1.264\n",
      "[Epoch 8, Batch   100] loss: 1.231\n",
      "[Epoch 8, Batch   200] loss: 1.220\n",
      "[Epoch 8, Batch   300] loss: 1.199\n",
      "[Epoch 8, Batch   400] loss: 1.227\n",
      "[Epoch 8, Batch   500] loss: 1.247\n",
      "[Epoch 8, Batch   600] loss: 1.219\n",
      "[Epoch 8, Batch   700] loss: 1.210\n",
      "[Epoch 8, Batch   800] loss: 1.231\n",
      "[Epoch 8, Batch   900] loss: 1.244\n",
      "[Epoch 8, Batch  1000] loss: 1.216\n",
      "[Epoch 8, Batch  1100] loss: 1.234\n",
      "[Epoch 8, Batch  1200] loss: 1.233\n",
      "[Epoch 8, Batch  1300] loss: 1.220\n",
      "[Epoch 8, Batch  1400] loss: 1.229\n",
      "[Epoch 8, Batch  1500] loss: 1.210\n",
      "[Epoch 9, Batch   100] loss: 1.175\n",
      "[Epoch 9, Batch   200] loss: 1.137\n",
      "[Epoch 9, Batch   300] loss: 1.212\n",
      "[Epoch 9, Batch   400] loss: 1.176\n",
      "[Epoch 9, Batch   500] loss: 1.199\n",
      "[Epoch 9, Batch   600] loss: 1.173\n",
      "[Epoch 9, Batch   700] loss: 1.163\n",
      "[Epoch 9, Batch   800] loss: 1.188\n",
      "[Epoch 9, Batch   900] loss: 1.194\n",
      "[Epoch 9, Batch  1000] loss: 1.154\n",
      "[Epoch 9, Batch  1100] loss: 1.168\n",
      "[Epoch 9, Batch  1200] loss: 1.201\n",
      "[Epoch 9, Batch  1300] loss: 1.186\n",
      "[Epoch 9, Batch  1400] loss: 1.203\n",
      "[Epoch 9, Batch  1500] loss: 1.186\n",
      "[Epoch 10, Batch   100] loss: 1.145\n",
      "[Epoch 10, Batch   200] loss: 1.188\n",
      "[Epoch 10, Batch   300] loss: 1.145\n",
      "[Epoch 10, Batch   400] loss: 1.150\n",
      "[Epoch 10, Batch   500] loss: 1.142\n",
      "[Epoch 10, Batch   600] loss: 1.163\n",
      "[Epoch 10, Batch   700] loss: 1.147\n",
      "[Epoch 10, Batch   800] loss: 1.156\n",
      "[Epoch 10, Batch   900] loss: 1.104\n",
      "[Epoch 10, Batch  1000] loss: 1.139\n",
      "[Epoch 10, Batch  1100] loss: 1.125\n",
      "[Epoch 10, Batch  1200] loss: 1.133\n",
      "[Epoch 10, Batch  1300] loss: 1.121\n",
      "[Epoch 10, Batch  1400] loss: 1.132\n",
      "[Epoch 10, Batch  1500] loss: 1.119\n",
      "[Epoch 11, Batch   100] loss: 1.116\n",
      "[Epoch 11, Batch   200] loss: 1.133\n",
      "[Epoch 11, Batch   300] loss: 1.062\n",
      "[Epoch 11, Batch   400] loss: 1.122\n",
      "[Epoch 11, Batch   500] loss: 1.112\n",
      "[Epoch 11, Batch   600] loss: 1.085\n",
      "[Epoch 11, Batch   700] loss: 1.111\n",
      "[Epoch 11, Batch   800] loss: 1.069\n",
      "[Epoch 11, Batch   900] loss: 1.133\n",
      "[Epoch 11, Batch  1000] loss: 1.099\n",
      "[Epoch 11, Batch  1100] loss: 1.129\n",
      "[Epoch 11, Batch  1200] loss: 1.074\n",
      "[Epoch 11, Batch  1300] loss: 1.089\n",
      "[Epoch 11, Batch  1400] loss: 1.111\n",
      "[Epoch 11, Batch  1500] loss: 1.107\n",
      "[Epoch 12, Batch   100] loss: 1.055\n",
      "[Epoch 12, Batch   200] loss: 1.074\n",
      "[Epoch 12, Batch   300] loss: 1.059\n",
      "[Epoch 12, Batch   400] loss: 1.094\n",
      "[Epoch 12, Batch   500] loss: 1.042\n",
      "[Epoch 12, Batch   600] loss: 1.062\n",
      "[Epoch 12, Batch   700] loss: 1.065\n",
      "[Epoch 12, Batch   800] loss: 1.065\n",
      "[Epoch 12, Batch   900] loss: 1.068\n",
      "[Epoch 12, Batch  1000] loss: 1.071\n",
      "[Epoch 12, Batch  1100] loss: 1.088\n",
      "[Epoch 12, Batch  1200] loss: 1.064\n",
      "[Epoch 12, Batch  1300] loss: 1.079\n",
      "[Epoch 12, Batch  1400] loss: 1.051\n",
      "[Epoch 12, Batch  1500] loss: 1.070\n",
      "[Epoch 13, Batch   100] loss: 1.060\n",
      "[Epoch 13, Batch   200] loss: 1.005\n",
      "[Epoch 13, Batch   300] loss: 1.039\n",
      "[Epoch 13, Batch   400] loss: 1.031\n",
      "[Epoch 13, Batch   500] loss: 1.049\n",
      "[Epoch 13, Batch   600] loss: 1.049\n",
      "[Epoch 13, Batch   700] loss: 1.050\n",
      "[Epoch 13, Batch   800] loss: 1.035\n",
      "[Epoch 13, Batch   900] loss: 1.050\n",
      "[Epoch 13, Batch  1000] loss: 1.033\n",
      "[Epoch 13, Batch  1100] loss: 1.025\n",
      "[Epoch 13, Batch  1200] loss: 1.026\n",
      "[Epoch 13, Batch  1300] loss: 1.043\n",
      "[Epoch 13, Batch  1400] loss: 1.045\n",
      "[Epoch 13, Batch  1500] loss: 1.025\n",
      "[Epoch 14, Batch   100] loss: 0.983\n",
      "[Epoch 14, Batch   200] loss: 0.997\n",
      "[Epoch 14, Batch   300] loss: 0.979\n",
      "[Epoch 14, Batch   400] loss: 0.976\n",
      "[Epoch 14, Batch   500] loss: 0.982\n",
      "[Epoch 14, Batch   600] loss: 1.027\n",
      "[Epoch 14, Batch   700] loss: 1.009\n",
      "[Epoch 14, Batch   800] loss: 1.011\n",
      "[Epoch 14, Batch   900] loss: 0.992\n",
      "[Epoch 14, Batch  1000] loss: 1.007\n",
      "[Epoch 14, Batch  1100] loss: 1.041\n",
      "[Epoch 14, Batch  1200] loss: 1.043\n",
      "[Epoch 14, Batch  1300] loss: 1.011\n",
      "[Epoch 14, Batch  1400] loss: 1.012\n",
      "[Epoch 14, Batch  1500] loss: 1.013\n",
      "[Epoch 15, Batch   100] loss: 0.968\n",
      "[Epoch 15, Batch   200] loss: 0.969\n",
      "[Epoch 15, Batch   300] loss: 0.992\n",
      "[Epoch 15, Batch   400] loss: 0.988\n",
      "[Epoch 15, Batch   500] loss: 0.960\n",
      "[Epoch 15, Batch   600] loss: 0.992\n",
      "[Epoch 15, Batch   700] loss: 0.986\n",
      "[Epoch 15, Batch   800] loss: 0.989\n",
      "[Epoch 15, Batch   900] loss: 0.983\n",
      "[Epoch 15, Batch  1000] loss: 0.965\n",
      "[Epoch 15, Batch  1100] loss: 0.974\n",
      "[Epoch 15, Batch  1200] loss: 0.984\n",
      "[Epoch 15, Batch  1300] loss: 1.000\n",
      "[Epoch 15, Batch  1400] loss: 1.003\n",
      "[Epoch 15, Batch  1500] loss: 0.971\n",
      "[Epoch 16, Batch   100] loss: 0.942\n",
      "[Epoch 16, Batch   200] loss: 0.971\n",
      "[Epoch 16, Batch   300] loss: 0.934\n",
      "[Epoch 16, Batch   400] loss: 0.980\n",
      "[Epoch 16, Batch   500] loss: 0.958\n",
      "[Epoch 16, Batch   600] loss: 0.931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16, Batch   700] loss: 0.942\n",
      "[Epoch 16, Batch   800] loss: 0.958\n",
      "[Epoch 16, Batch   900] loss: 0.920\n",
      "[Epoch 16, Batch  1000] loss: 0.971\n",
      "[Epoch 16, Batch  1100] loss: 0.931\n",
      "[Epoch 16, Batch  1200] loss: 0.944\n",
      "[Epoch 16, Batch  1300] loss: 0.964\n",
      "[Epoch 16, Batch  1400] loss: 0.948\n",
      "[Epoch 16, Batch  1500] loss: 0.961\n",
      "[Epoch 17, Batch   100] loss: 0.914\n",
      "[Epoch 17, Batch   200] loss: 0.946\n",
      "[Epoch 17, Batch   300] loss: 0.914\n",
      "[Epoch 17, Batch   400] loss: 0.925\n",
      "[Epoch 17, Batch   500] loss: 0.901\n",
      "[Epoch 17, Batch   600] loss: 0.938\n",
      "[Epoch 17, Batch   700] loss: 0.967\n",
      "[Epoch 17, Batch   800] loss: 0.935\n",
      "[Epoch 17, Batch   900] loss: 0.915\n",
      "[Epoch 17, Batch  1000] loss: 0.903\n",
      "[Epoch 17, Batch  1100] loss: 0.944\n",
      "[Epoch 17, Batch  1200] loss: 0.921\n",
      "[Epoch 17, Batch  1300] loss: 0.936\n",
      "[Epoch 17, Batch  1400] loss: 0.949\n",
      "[Epoch 17, Batch  1500] loss: 0.921\n",
      "[Epoch 18, Batch   100] loss: 0.903\n",
      "[Epoch 18, Batch   200] loss: 0.886\n",
      "[Epoch 18, Batch   300] loss: 0.898\n",
      "[Epoch 18, Batch   400] loss: 0.889\n",
      "[Epoch 18, Batch   500] loss: 0.906\n",
      "[Epoch 18, Batch   600] loss: 0.901\n",
      "[Epoch 18, Batch   700] loss: 0.894\n",
      "[Epoch 18, Batch   800] loss: 0.926\n",
      "[Epoch 18, Batch   900] loss: 0.893\n",
      "[Epoch 18, Batch  1000] loss: 0.913\n",
      "[Epoch 18, Batch  1100] loss: 0.885\n",
      "[Epoch 18, Batch  1200] loss: 0.912\n",
      "[Epoch 18, Batch  1300] loss: 0.892\n",
      "[Epoch 18, Batch  1400] loss: 0.906\n",
      "[Epoch 18, Batch  1500] loss: 0.916\n",
      "[Epoch 19, Batch   100] loss: 0.839\n",
      "[Epoch 19, Batch   200] loss: 0.877\n",
      "[Epoch 19, Batch   300] loss: 0.889\n",
      "[Epoch 19, Batch   400] loss: 0.856\n",
      "[Epoch 19, Batch   500] loss: 0.906\n",
      "[Epoch 19, Batch   600] loss: 0.866\n",
      "[Epoch 19, Batch   700] loss: 0.885\n",
      "[Epoch 19, Batch   800] loss: 0.877\n",
      "[Epoch 19, Batch   900] loss: 0.895\n",
      "[Epoch 19, Batch  1000] loss: 0.880\n",
      "[Epoch 19, Batch  1100] loss: 0.876\n",
      "[Epoch 19, Batch  1200] loss: 0.896\n",
      "[Epoch 19, Batch  1300] loss: 0.902\n",
      "[Epoch 19, Batch  1400] loss: 0.894\n",
      "[Epoch 19, Batch  1500] loss: 0.904\n",
      "[Epoch 20, Batch   100] loss: 0.867\n",
      "[Epoch 20, Batch   200] loss: 0.848\n",
      "[Epoch 20, Batch   300] loss: 0.830\n",
      "[Epoch 20, Batch   400] loss: 0.812\n",
      "[Epoch 20, Batch   500] loss: 0.860\n",
      "[Epoch 20, Batch   600] loss: 0.856\n",
      "[Epoch 20, Batch   700] loss: 0.838\n",
      "[Epoch 20, Batch   800] loss: 0.899\n",
      "[Epoch 20, Batch   900] loss: 0.838\n",
      "[Epoch 20, Batch  1000] loss: 0.833\n",
      "[Epoch 20, Batch  1100] loss: 0.884\n",
      "[Epoch 20, Batch  1200] loss: 0.870\n",
      "[Epoch 20, Batch  1300] loss: 0.867\n",
      "[Epoch 20, Batch  1400] loss: 0.872\n",
      "[Epoch 20, Batch  1500] loss: 0.899\n",
      "[Epoch 21, Batch   100] loss: 0.797\n",
      "[Epoch 21, Batch   200] loss: 0.801\n",
      "[Epoch 21, Batch   300] loss: 0.785\n",
      "[Epoch 21, Batch   400] loss: 0.855\n",
      "[Epoch 21, Batch   500] loss: 0.844\n",
      "[Epoch 21, Batch   600] loss: 0.846\n",
      "[Epoch 21, Batch   700] loss: 0.849\n",
      "[Epoch 21, Batch   800] loss: 0.854\n",
      "[Epoch 21, Batch   900] loss: 0.830\n",
      "[Epoch 21, Batch  1000] loss: 0.832\n",
      "[Epoch 21, Batch  1100] loss: 0.833\n",
      "[Epoch 21, Batch  1200] loss: 0.832\n",
      "[Epoch 21, Batch  1300] loss: 0.858\n",
      "[Epoch 21, Batch  1400] loss: 0.853\n",
      "[Epoch 21, Batch  1500] loss: 0.852\n",
      "[Epoch 22, Batch   100] loss: 0.802\n",
      "[Epoch 22, Batch   200] loss: 0.831\n",
      "[Epoch 22, Batch   300] loss: 0.831\n",
      "[Epoch 22, Batch   400] loss: 0.794\n",
      "[Epoch 22, Batch   500] loss: 0.795\n",
      "[Epoch 22, Batch   600] loss: 0.852\n",
      "[Epoch 22, Batch   700] loss: 0.801\n",
      "[Epoch 22, Batch   800] loss: 0.819\n",
      "[Epoch 22, Batch   900] loss: 0.811\n",
      "[Epoch 22, Batch  1000] loss: 0.802\n",
      "[Epoch 22, Batch  1100] loss: 0.802\n",
      "[Epoch 22, Batch  1200] loss: 0.794\n",
      "[Epoch 22, Batch  1300] loss: 0.839\n",
      "[Epoch 22, Batch  1400] loss: 0.841\n",
      "[Epoch 22, Batch  1500] loss: 0.809\n",
      "[Epoch 23, Batch   100] loss: 0.770\n",
      "[Epoch 23, Batch   200] loss: 0.782\n",
      "[Epoch 23, Batch   300] loss: 0.785\n",
      "[Epoch 23, Batch   400] loss: 0.809\n",
      "[Epoch 23, Batch   500] loss: 0.782\n",
      "[Epoch 23, Batch   600] loss: 0.759\n",
      "[Epoch 23, Batch   700] loss: 0.798\n",
      "[Epoch 23, Batch   800] loss: 0.788\n",
      "[Epoch 23, Batch   900] loss: 0.780\n",
      "[Epoch 23, Batch  1000] loss: 0.832\n",
      "[Epoch 23, Batch  1100] loss: 0.838\n",
      "[Epoch 23, Batch  1200] loss: 0.795\n",
      "[Epoch 23, Batch  1300] loss: 0.802\n",
      "[Epoch 23, Batch  1400] loss: 0.808\n",
      "[Epoch 23, Batch  1500] loss: 0.779\n",
      "[Epoch 24, Batch   100] loss: 0.740\n",
      "[Epoch 24, Batch   200] loss: 0.753\n",
      "[Epoch 24, Batch   300] loss: 0.757\n",
      "[Epoch 24, Batch   400] loss: 0.752\n",
      "[Epoch 24, Batch   500] loss: 0.782\n",
      "[Epoch 24, Batch   600] loss: 0.750\n",
      "[Epoch 24, Batch   700] loss: 0.793\n",
      "[Epoch 24, Batch   800] loss: 0.771\n",
      "[Epoch 24, Batch   900] loss: 0.767\n",
      "[Epoch 24, Batch  1000] loss: 0.757\n",
      "[Epoch 24, Batch  1100] loss: 0.825\n",
      "[Epoch 24, Batch  1200] loss: 0.789\n",
      "[Epoch 24, Batch  1300] loss: 0.789\n",
      "[Epoch 24, Batch  1400] loss: 0.807\n",
      "[Epoch 24, Batch  1500] loss: 0.816\n",
      "[Epoch 25, Batch   100] loss: 0.704\n",
      "[Epoch 25, Batch   200] loss: 0.737\n",
      "[Epoch 25, Batch   300] loss: 0.738\n",
      "[Epoch 25, Batch   400] loss: 0.744\n",
      "[Epoch 25, Batch   500] loss: 0.758\n",
      "[Epoch 25, Batch   600] loss: 0.750\n",
      "[Epoch 25, Batch   700] loss: 0.760\n",
      "[Epoch 25, Batch   800] loss: 0.826\n",
      "[Epoch 25, Batch   900] loss: 0.732\n",
      "[Epoch 25, Batch  1000] loss: 0.735\n",
      "[Epoch 25, Batch  1100] loss: 0.750\n",
      "[Epoch 25, Batch  1200] loss: 0.746\n",
      "[Epoch 25, Batch  1300] loss: 0.764\n",
      "[Epoch 25, Batch  1400] loss: 0.803\n",
      "[Epoch 25, Batch  1500] loss: 0.798\n",
      "[Epoch 26, Batch   100] loss: 0.682\n",
      "[Epoch 26, Batch   200] loss: 0.738\n",
      "[Epoch 26, Batch   300] loss: 0.727\n",
      "[Epoch 26, Batch   400] loss: 0.718\n",
      "[Epoch 26, Batch   500] loss: 0.715\n",
      "[Epoch 26, Batch   600] loss: 0.740\n",
      "[Epoch 26, Batch   700] loss: 0.734\n",
      "[Epoch 26, Batch   800] loss: 0.780\n",
      "[Epoch 26, Batch   900] loss: 0.777\n",
      "[Epoch 26, Batch  1000] loss: 0.730\n",
      "[Epoch 26, Batch  1100] loss: 0.737\n",
      "[Epoch 26, Batch  1200] loss: 0.760\n",
      "[Epoch 26, Batch  1300] loss: 0.724\n",
      "[Epoch 26, Batch  1400] loss: 0.777\n",
      "[Epoch 26, Batch  1500] loss: 0.764\n",
      "[Epoch 27, Batch   100] loss: 0.715\n",
      "[Epoch 27, Batch   200] loss: 0.671\n",
      "[Epoch 27, Batch   300] loss: 0.679\n",
      "[Epoch 27, Batch   400] loss: 0.747\n",
      "[Epoch 27, Batch   500] loss: 0.746\n",
      "[Epoch 27, Batch   600] loss: 0.742\n",
      "[Epoch 27, Batch   700] loss: 0.713\n",
      "[Epoch 27, Batch   800] loss: 0.733\n",
      "[Epoch 27, Batch   900] loss: 0.720\n",
      "[Epoch 27, Batch  1000] loss: 0.729\n",
      "[Epoch 27, Batch  1100] loss: 0.715\n",
      "[Epoch 27, Batch  1200] loss: 0.742\n",
      "[Epoch 27, Batch  1300] loss: 0.749\n",
      "[Epoch 27, Batch  1400] loss: 0.697\n",
      "[Epoch 27, Batch  1500] loss: 0.752\n",
      "[Epoch 28, Batch   100] loss: 0.667\n",
      "[Epoch 28, Batch   200] loss: 0.680\n",
      "[Epoch 28, Batch   300] loss: 0.698\n",
      "[Epoch 28, Batch   400] loss: 0.693\n",
      "[Epoch 28, Batch   500] loss: 0.705\n",
      "[Epoch 28, Batch   600] loss: 0.729\n",
      "[Epoch 28, Batch   700] loss: 0.676\n",
      "[Epoch 28, Batch   800] loss: 0.670\n",
      "[Epoch 28, Batch   900] loss: 0.717\n",
      "[Epoch 28, Batch  1000] loss: 0.709\n",
      "[Epoch 28, Batch  1100] loss: 0.711\n",
      "[Epoch 28, Batch  1200] loss: 0.700\n",
      "[Epoch 28, Batch  1300] loss: 0.700\n",
      "[Epoch 28, Batch  1400] loss: 0.733\n",
      "[Epoch 28, Batch  1500] loss: 0.775\n",
      "[Epoch 29, Batch   100] loss: 0.648\n",
      "[Epoch 29, Batch   200] loss: 0.663\n",
      "[Epoch 29, Batch   300] loss: 0.641\n",
      "[Epoch 29, Batch   400] loss: 0.684\n",
      "[Epoch 29, Batch   500] loss: 0.668\n",
      "[Epoch 29, Batch   600] loss: 0.706\n",
      "[Epoch 29, Batch   700] loss: 0.687\n",
      "[Epoch 29, Batch   800] loss: 0.684\n",
      "[Epoch 29, Batch   900] loss: 0.698\n",
      "[Epoch 29, Batch  1000] loss: 0.707\n",
      "[Epoch 29, Batch  1100] loss: 0.700\n",
      "[Epoch 29, Batch  1200] loss: 0.697\n",
      "[Epoch 29, Batch  1300] loss: 0.720\n",
      "[Epoch 29, Batch  1400] loss: 0.696\n",
      "[Epoch 29, Batch  1500] loss: 0.719\n",
      "[Epoch 30, Batch   100] loss: 0.605\n",
      "[Epoch 30, Batch   200] loss: 0.649\n",
      "[Epoch 30, Batch   300] loss: 0.633\n",
      "[Epoch 30, Batch   400] loss: 0.694\n",
      "[Epoch 30, Batch   500] loss: 0.681\n",
      "[Epoch 30, Batch   600] loss: 0.672\n",
      "[Epoch 30, Batch   700] loss: 0.676\n",
      "[Epoch 30, Batch   800] loss: 0.688\n",
      "[Epoch 30, Batch   900] loss: 0.654\n",
      "[Epoch 30, Batch  1000] loss: 0.677\n",
      "[Epoch 30, Batch  1100] loss: 0.680\n",
      "[Epoch 30, Batch  1200] loss: 0.663\n",
      "[Epoch 30, Batch  1300] loss: 0.716\n",
      "[Epoch 30, Batch  1400] loss: 0.700\n",
      "[Epoch 30, Batch  1500] loss: 0.733\n",
      "Done Training!\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Training...\")\n",
    "for epoch in range(30):\n",
    "    # \n",
    "    loss100 = 0.0\n",
    "    # \n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device) # \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        # print(outputs.shape)\n",
    "        # print(labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss100 += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[Epoch %d, Batch %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, loss100 / 100))\n",
    "            loss100 = 0.0\n",
    "\n",
    "print(\"Done Training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 60 %\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "dataiter = iter(testloader)\n",
    "# \n",
    "correct = 0\n",
    "total = 0\n",
    "# \n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # \n",
    "        outputs = net(images)\n",
    "        # \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testMap = []\n",
    "labelMap = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        outputs_tocsv = outputs.cpu().data.numpy()\n",
    "        labels_ = labels.cpu().data.numpy()\n",
    "        \n",
    "        # outputs_tocsv = outputs_tocsv[0]\n",
    "        testMap = testMap+list(outputs_tocsv)\n",
    "        labelMap = labelMap+ list(labels_)\n",
    "        # print(labelMap)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "testMap = list(testMap)\n",
    "labelMap = list(labelMap)\n",
    "# print(testMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.savetxt('testMap_vgg16_60.csv', testMap, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.savetxt('labelMap_vgg16_60.csv', labelMap, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b8513658d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "image1 = cifar_test[0]\n",
    "image1 = image1[0].numpy()\n",
    "image1 = numpy.moveaxis(image1, 0, -1)\n",
    "print(image1.shape)\n",
    "plt.imshow(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.010426  -2.5197885  2.265055   1.0261331 -0.4024902  3.7831528\n",
      " -2.9883025  2.5635188 -4.9304595 -1.042607 ]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "testloader = torch.utils.data.DataLoader(cifar_test, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        outputs_tocsv = outputs.cpu().data.numpy()\n",
    "        outputs_tocsv = outputs_tocsv[0]\n",
    "        print(outputs_tocsv) # dog\n",
    "        outputs_tocsv = outputs_tocsv.tolist()\n",
    "        index = outputs_tocsv.index(max(outputs_tocsv)) \n",
    "        print(index)\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(cifar_train, batch_size=1, shuffle=True)\n",
    "retrieval_image = []\n",
    "with torch.no_grad():\n",
    "    for data in trainloader: \n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels=labels.cpu().data.numpy()\n",
    "        labels = labels[0]\n",
    "        if labels == index:\n",
    "            images=images.cpu().data.numpy()\n",
    "            images = images[0]\n",
    "            images = numpy.moveaxis(images, 0, -1)\n",
    "            plt.imshow(images)\n",
    "            retrieval_image.append(images)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
