{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "# cifar-10官方提供的数据集是用numpy array存储的\n",
    "# 下面这个transform会把numpy array变成torch tensor，然后把rgb值归一到[0, 1]这个区间\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# 在构建数据集的时候指定transform，就会应用我们定义好的transform\n",
    "# root是存储数据的文件夹，download=True指定如果数据不存在先下载数据\n",
    "cifar_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                           download=True, transform=transform)\n",
    "cifar_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                          transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Split: train\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "print(cifar_train)\n",
    "print(cifar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(cifar_train, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(cifar_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    # 一般在__init__中定义网络需要的操作算子，比如卷积、全连接算子等等\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # Conv2d的第一个参数是输入的channel数量，第二个是输出的channel数量，第三个是kernel size\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # 由于上一层有16个channel输出，每个feature map大小为5*5，所以全连接层的输入是16*5*5\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # 最终有10类，所以最后一个全连接层输出数量是10\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    # forward这个函数定义了前向传播的运算，只需要像写普通的python算数运算那样就可以了\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        # 下面这步把二维特征图变为一维，这样全连接层才能处理\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "net = LeNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim中定义了各种各样的优化方法，包括SGD\n",
    "import torch.optim as optim\n",
    "\n",
    "# CrossEntropyLoss就是我们需要的损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "Done Training!\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Training...\")\n",
    "for epoch in range(30):\n",
    "    # 我们用一个变量来记录每100个batch的平均loss\n",
    "    loss100 = 0.0\n",
    "    # 我们的dataloader派上了用场\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device) # 注意需要复制到GPU\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        print(outputs.shape)\n",
    "        # print(labels)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss100 += loss.item()\n",
    "        break\n",
    "        if i % 100 == 99:\n",
    "            print('[Epoch %d, Batch %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, loss100 / 100))\n",
    "            loss100 = 0.0\n",
    "\n",
    "print(\"Done Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        # conv layers: (in_channel size, out_channels size, kernel_size, stride, padding)\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        # max pooling (kernel_size, stride)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # fully conected layers:\n",
    "        self.fc6 = nn.Linear(16384, 4096)\n",
    "        self.fc7 = nn.Linear(4096, 4096)\n",
    "        self.fc8 = nn.Linear(4096, 1000)\n",
    "        self.fc9 = nn.Linear(1000, 100)\n",
    "        self.fc10 = nn.Linear(100, 10)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = F.relu(self.conv3_2(x))\n",
    "        x = F.relu(self.conv3_3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv4_1(x))\n",
    "        x = F.relu(self.conv4_2(x))\n",
    "        x = F.relu(self.conv4_3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv5_1(x))\n",
    "        x = F.relu(self.conv5_2(x))\n",
    "        x = F.relu(self.conv5_3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 16384)\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc8(x)\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc9(x)\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc10(x)\n",
    "\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(cifar_train, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(cifar_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "net = VGG16().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropyLoss就是我们需要的损失函数\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "[Epoch 1, Batch   100] loss: 0.294\n",
      "[Epoch 1, Batch   200] loss: 0.259\n",
      "[Epoch 1, Batch   300] loss: 0.283\n",
      "[Epoch 1, Batch   400] loss: 0.299\n",
      "[Epoch 1, Batch   500] loss: 0.304\n",
      "[Epoch 1, Batch   600] loss: 0.304\n",
      "[Epoch 1, Batch   700] loss: 0.313\n",
      "[Epoch 1, Batch   800] loss: 0.328\n",
      "[Epoch 1, Batch   900] loss: 0.331\n",
      "[Epoch 1, Batch  1000] loss: 0.361\n",
      "[Epoch 1, Batch  1100] loss: 0.355\n",
      "[Epoch 1, Batch  1200] loss: 0.363\n",
      "[Epoch 1, Batch  1300] loss: 0.352\n",
      "[Epoch 1, Batch  1400] loss: 0.369\n",
      "[Epoch 1, Batch  1500] loss: 0.384\n",
      "[Epoch 2, Batch   100] loss: 0.285\n",
      "[Epoch 2, Batch   200] loss: 0.292\n",
      "[Epoch 2, Batch   300] loss: 0.271\n",
      "[Epoch 2, Batch   400] loss: 0.299\n",
      "[Epoch 2, Batch   500] loss: 0.320\n",
      "[Epoch 2, Batch   600] loss: 0.316\n",
      "[Epoch 2, Batch   700] loss: 0.308\n",
      "[Epoch 2, Batch   800] loss: 0.302\n",
      "[Epoch 2, Batch   900] loss: 0.330\n",
      "[Epoch 2, Batch  1000] loss: 0.347\n",
      "[Epoch 2, Batch  1100] loss: 0.341\n",
      "[Epoch 2, Batch  1200] loss: 0.362\n",
      "[Epoch 2, Batch  1300] loss: 0.349\n",
      "[Epoch 2, Batch  1400] loss: 0.338\n",
      "[Epoch 2, Batch  1500] loss: 0.376\n",
      "[Epoch 3, Batch   100] loss: 0.269\n",
      "[Epoch 3, Batch   200] loss: 0.288\n",
      "[Epoch 3, Batch   300] loss: 0.272\n",
      "[Epoch 3, Batch   400] loss: 0.286\n",
      "[Epoch 3, Batch   500] loss: 0.277\n",
      "[Epoch 3, Batch   600] loss: 0.283\n",
      "[Epoch 3, Batch   700] loss: 0.318\n",
      "[Epoch 3, Batch   800] loss: 0.313\n",
      "[Epoch 3, Batch   900] loss: 0.316\n",
      "[Epoch 3, Batch  1000] loss: 0.346\n",
      "[Epoch 3, Batch  1100] loss: 0.369\n",
      "[Epoch 3, Batch  1200] loss: 0.338\n",
      "[Epoch 3, Batch  1300] loss: 0.337\n",
      "[Epoch 3, Batch  1400] loss: 0.377\n",
      "[Epoch 3, Batch  1500] loss: 0.405\n",
      "[Epoch 4, Batch   100] loss: 0.271\n",
      "[Epoch 4, Batch   200] loss: 0.259\n",
      "[Epoch 4, Batch   300] loss: 0.280\n",
      "[Epoch 4, Batch   400] loss: 0.273\n",
      "[Epoch 4, Batch   500] loss: 0.283\n",
      "[Epoch 4, Batch   600] loss: 0.306\n",
      "[Epoch 4, Batch   700] loss: 0.296\n",
      "[Epoch 4, Batch   800] loss: 0.323\n",
      "[Epoch 4, Batch   900] loss: 0.330\n",
      "[Epoch 4, Batch  1000] loss: 0.338\n",
      "[Epoch 4, Batch  1100] loss: 0.309\n",
      "[Epoch 4, Batch  1200] loss: 0.327\n",
      "[Epoch 4, Batch  1300] loss: 0.344\n",
      "[Epoch 4, Batch  1400] loss: 0.373\n",
      "[Epoch 4, Batch  1500] loss: 0.329\n",
      "[Epoch 5, Batch   100] loss: 0.248\n",
      "[Epoch 5, Batch   200] loss: 0.274\n",
      "[Epoch 5, Batch   300] loss: 0.249\n",
      "[Epoch 5, Batch   400] loss: 0.276\n",
      "[Epoch 5, Batch   500] loss: 0.266\n",
      "[Epoch 5, Batch   600] loss: 0.272\n",
      "[Epoch 5, Batch   700] loss: 0.318\n",
      "[Epoch 5, Batch   800] loss: 0.319\n",
      "[Epoch 5, Batch   900] loss: 0.312\n",
      "[Epoch 5, Batch  1000] loss: 0.288\n",
      "[Epoch 5, Batch  1100] loss: 0.315\n",
      "[Epoch 5, Batch  1200] loss: 0.362\n",
      "[Epoch 5, Batch  1300] loss: 0.364\n",
      "[Epoch 5, Batch  1400] loss: 0.293\n",
      "[Epoch 5, Batch  1500] loss: 0.362\n",
      "[Epoch 6, Batch   100] loss: 0.235\n",
      "[Epoch 6, Batch   200] loss: 0.251\n",
      "[Epoch 6, Batch   300] loss: 0.228\n",
      "[Epoch 6, Batch   400] loss: 0.245\n",
      "[Epoch 6, Batch   500] loss: 0.296\n",
      "[Epoch 6, Batch   600] loss: 0.288\n",
      "[Epoch 6, Batch   700] loss: 0.283\n",
      "[Epoch 6, Batch   800] loss: 0.293\n",
      "[Epoch 6, Batch   900] loss: 0.288\n",
      "[Epoch 6, Batch  1000] loss: 0.305\n",
      "[Epoch 6, Batch  1100] loss: 0.334\n",
      "[Epoch 6, Batch  1200] loss: 0.325\n",
      "[Epoch 6, Batch  1300] loss: 0.320\n",
      "[Epoch 6, Batch  1400] loss: 0.342\n",
      "[Epoch 6, Batch  1500] loss: 0.324\n",
      "[Epoch 7, Batch   100] loss: 0.244\n",
      "[Epoch 7, Batch   200] loss: 0.248\n",
      "[Epoch 7, Batch   300] loss: 0.259\n",
      "[Epoch 7, Batch   400] loss: 0.240\n",
      "[Epoch 7, Batch   500] loss: 0.246\n",
      "[Epoch 7, Batch   600] loss: 0.287\n",
      "[Epoch 7, Batch   700] loss: 0.280\n",
      "[Epoch 7, Batch   800] loss: 0.257\n",
      "[Epoch 7, Batch   900] loss: 0.318\n",
      "[Epoch 7, Batch  1000] loss: 0.282\n",
      "[Epoch 7, Batch  1100] loss: 0.313\n",
      "[Epoch 7, Batch  1200] loss: 0.335\n",
      "[Epoch 7, Batch  1300] loss: 0.326\n",
      "[Epoch 7, Batch  1400] loss: 0.370\n",
      "[Epoch 7, Batch  1500] loss: 0.359\n",
      "[Epoch 8, Batch   100] loss: 0.230\n",
      "[Epoch 8, Batch   200] loss: 0.233\n",
      "[Epoch 8, Batch   300] loss: 0.262\n",
      "[Epoch 8, Batch   400] loss: 0.245\n",
      "[Epoch 8, Batch   500] loss: 0.262\n",
      "[Epoch 8, Batch   600] loss: 0.254\n",
      "[Epoch 8, Batch   700] loss: 0.277\n",
      "[Epoch 8, Batch   800] loss: 0.277\n",
      "[Epoch 8, Batch   900] loss: 0.327\n",
      "[Epoch 8, Batch  1000] loss: 0.294\n",
      "[Epoch 8, Batch  1100] loss: 0.307\n",
      "[Epoch 8, Batch  1200] loss: 0.332\n",
      "[Epoch 8, Batch  1300] loss: 0.353\n",
      "[Epoch 8, Batch  1400] loss: 0.303\n",
      "[Epoch 8, Batch  1500] loss: 0.345\n",
      "[Epoch 9, Batch   100] loss: 0.241\n",
      "[Epoch 9, Batch   200] loss: 0.244\n",
      "[Epoch 9, Batch   300] loss: 0.240\n",
      "[Epoch 9, Batch   400] loss: 0.299\n",
      "[Epoch 9, Batch   500] loss: 0.279\n",
      "[Epoch 9, Batch   600] loss: 0.246\n",
      "[Epoch 9, Batch   700] loss: 0.298\n",
      "[Epoch 9, Batch   800] loss: 0.286\n",
      "[Epoch 9, Batch   900] loss: 0.261\n",
      "[Epoch 9, Batch  1000] loss: 0.275\n",
      "[Epoch 9, Batch  1100] loss: 0.299\n",
      "[Epoch 9, Batch  1200] loss: 0.266\n",
      "[Epoch 9, Batch  1300] loss: 0.300\n",
      "[Epoch 9, Batch  1400] loss: 0.296\n",
      "[Epoch 9, Batch  1500] loss: 0.339\n",
      "[Epoch 10, Batch   100] loss: 0.234\n",
      "[Epoch 10, Batch   200] loss: 0.251\n",
      "[Epoch 10, Batch   300] loss: 0.255\n",
      "[Epoch 10, Batch   400] loss: 0.258\n",
      "[Epoch 10, Batch   500] loss: 0.255\n",
      "[Epoch 10, Batch   600] loss: 0.290\n",
      "[Epoch 10, Batch   700] loss: 0.250\n",
      "[Epoch 10, Batch   800] loss: 0.285\n",
      "[Epoch 10, Batch   900] loss: 0.297\n",
      "[Epoch 10, Batch  1000] loss: 0.278\n",
      "[Epoch 10, Batch  1100] loss: 0.323\n",
      "[Epoch 10, Batch  1200] loss: 0.321\n",
      "[Epoch 10, Batch  1300] loss: 0.334\n",
      "[Epoch 10, Batch  1400] loss: 0.345\n",
      "[Epoch 10, Batch  1500] loss: 0.343\n",
      "[Epoch 11, Batch   100] loss: 0.224\n",
      "[Epoch 11, Batch   200] loss: 0.218\n",
      "[Epoch 11, Batch   300] loss: 0.255\n",
      "[Epoch 11, Batch   400] loss: 0.257\n",
      "[Epoch 11, Batch   500] loss: 0.278\n",
      "[Epoch 11, Batch   600] loss: 0.246\n",
      "[Epoch 11, Batch   700] loss: 0.256\n",
      "[Epoch 11, Batch   800] loss: 0.279\n",
      "[Epoch 11, Batch   900] loss: 0.280\n",
      "[Epoch 11, Batch  1000] loss: 0.273\n",
      "[Epoch 11, Batch  1100] loss: 0.294\n",
      "[Epoch 11, Batch  1200] loss: 0.287\n",
      "[Epoch 11, Batch  1300] loss: 0.302\n",
      "[Epoch 11, Batch  1400] loss: 0.320\n",
      "[Epoch 11, Batch  1500] loss: 0.323\n",
      "[Epoch 12, Batch   100] loss: 0.284\n",
      "[Epoch 12, Batch   200] loss: 0.241\n",
      "[Epoch 12, Batch   300] loss: 0.194\n",
      "[Epoch 12, Batch   400] loss: 0.222\n",
      "[Epoch 12, Batch   500] loss: 0.236\n",
      "[Epoch 12, Batch   600] loss: 0.247\n",
      "[Epoch 12, Batch   700] loss: 0.241\n",
      "[Epoch 12, Batch   800] loss: 0.274\n",
      "[Epoch 12, Batch   900] loss: 0.314\n",
      "[Epoch 12, Batch  1000] loss: 0.336\n",
      "[Epoch 12, Batch  1100] loss: 0.324\n",
      "[Epoch 12, Batch  1200] loss: 0.296\n",
      "[Epoch 12, Batch  1300] loss: 0.288\n",
      "[Epoch 12, Batch  1400] loss: 0.293\n",
      "[Epoch 12, Batch  1500] loss: 0.314\n",
      "[Epoch 13, Batch   100] loss: 0.221\n",
      "[Epoch 13, Batch   200] loss: 0.229\n",
      "[Epoch 13, Batch   300] loss: 0.203\n",
      "[Epoch 13, Batch   400] loss: 0.242\n",
      "[Epoch 13, Batch   500] loss: 0.292\n",
      "[Epoch 13, Batch   600] loss: 0.302\n",
      "[Epoch 13, Batch   700] loss: 0.295\n",
      "[Epoch 13, Batch   800] loss: 0.258\n",
      "[Epoch 13, Batch   900] loss: 0.277\n",
      "[Epoch 13, Batch  1000] loss: 0.258\n",
      "[Epoch 13, Batch  1100] loss: 0.284\n",
      "[Epoch 13, Batch  1200] loss: 0.313\n",
      "[Epoch 13, Batch  1300] loss: 0.294\n",
      "[Epoch 13, Batch  1400] loss: 0.268\n",
      "[Epoch 13, Batch  1500] loss: 0.294\n",
      "[Epoch 14, Batch   100] loss: 0.205\n",
      "[Epoch 14, Batch   200] loss: 0.208\n",
      "[Epoch 14, Batch   300] loss: 0.230\n",
      "[Epoch 14, Batch   400] loss: 0.224\n",
      "[Epoch 14, Batch   500] loss: 0.254\n",
      "[Epoch 14, Batch   600] loss: 0.243\n",
      "[Epoch 14, Batch   700] loss: 0.243\n",
      "[Epoch 14, Batch   800] loss: 0.244\n",
      "[Epoch 14, Batch   900] loss: 0.258\n",
      "[Epoch 14, Batch  1000] loss: 0.294\n",
      "[Epoch 14, Batch  1100] loss: 0.256\n",
      "[Epoch 14, Batch  1200] loss: 0.263\n",
      "[Epoch 14, Batch  1300] loss: 0.294\n",
      "[Epoch 14, Batch  1400] loss: 0.299\n",
      "[Epoch 14, Batch  1500] loss: 0.275\n",
      "[Epoch 15, Batch   100] loss: 0.236\n",
      "[Epoch 15, Batch   200] loss: 0.194\n",
      "[Epoch 15, Batch   300] loss: 0.218\n",
      "[Epoch 15, Batch   400] loss: 0.207\n",
      "[Epoch 15, Batch   500] loss: 0.234\n",
      "[Epoch 15, Batch   600] loss: 0.246\n",
      "[Epoch 15, Batch   700] loss: 0.248\n",
      "[Epoch 15, Batch   800] loss: 0.276\n",
      "[Epoch 15, Batch   900] loss: 0.263\n",
      "[Epoch 15, Batch  1000] loss: 0.249\n",
      "[Epoch 15, Batch  1100] loss: 0.253\n",
      "[Epoch 15, Batch  1200] loss: 0.271\n",
      "[Epoch 15, Batch  1300] loss: 0.279\n",
      "[Epoch 15, Batch  1400] loss: 0.300\n",
      "[Epoch 15, Batch  1500] loss: 0.304\n",
      "[Epoch 16, Batch   100] loss: 0.214\n",
      "[Epoch 16, Batch   200] loss: 0.191\n",
      "[Epoch 16, Batch   300] loss: 0.191\n",
      "[Epoch 16, Batch   400] loss: 0.214\n",
      "[Epoch 16, Batch   500] loss: 0.211\n",
      "[Epoch 16, Batch   600] loss: 0.204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16, Batch   700] loss: 0.224\n",
      "[Epoch 16, Batch   800] loss: 0.238\n",
      "[Epoch 16, Batch   900] loss: 0.269\n",
      "[Epoch 16, Batch  1000] loss: 0.266\n",
      "[Epoch 16, Batch  1100] loss: 0.264\n",
      "[Epoch 16, Batch  1200] loss: 0.262\n",
      "[Epoch 16, Batch  1300] loss: 0.316\n",
      "[Epoch 16, Batch  1400] loss: 0.307\n",
      "[Epoch 16, Batch  1500] loss: 0.300\n",
      "[Epoch 17, Batch   100] loss: 0.199\n",
      "[Epoch 17, Batch   200] loss: 0.200\n",
      "[Epoch 17, Batch   300] loss: 0.228\n",
      "[Epoch 17, Batch   400] loss: 0.229\n",
      "[Epoch 17, Batch   500] loss: 0.217\n",
      "[Epoch 17, Batch   600] loss: 0.216\n",
      "[Epoch 17, Batch   700] loss: 0.252\n",
      "[Epoch 17, Batch   800] loss: 0.256\n",
      "[Epoch 17, Batch   900] loss: 0.273\n",
      "[Epoch 17, Batch  1000] loss: 0.292\n",
      "[Epoch 17, Batch  1100] loss: 0.259\n",
      "[Epoch 17, Batch  1200] loss: 0.263\n",
      "[Epoch 17, Batch  1300] loss: 0.302\n",
      "[Epoch 17, Batch  1400] loss: 0.275\n",
      "[Epoch 17, Batch  1500] loss: 0.305\n",
      "[Epoch 18, Batch   100] loss: 0.198\n",
      "[Epoch 18, Batch   200] loss: 0.192\n",
      "[Epoch 18, Batch   300] loss: 0.190\n",
      "[Epoch 18, Batch   400] loss: 0.223\n",
      "[Epoch 18, Batch   500] loss: 0.212\n",
      "[Epoch 18, Batch   600] loss: 0.231\n",
      "[Epoch 18, Batch   700] loss: 0.216\n",
      "[Epoch 18, Batch   800] loss: 0.249\n",
      "[Epoch 18, Batch   900] loss: 0.239\n",
      "[Epoch 18, Batch  1000] loss: 0.234\n",
      "[Epoch 18, Batch  1100] loss: 0.263\n",
      "[Epoch 18, Batch  1200] loss: 0.219\n",
      "[Epoch 18, Batch  1300] loss: 0.255\n",
      "[Epoch 18, Batch  1400] loss: 0.287\n",
      "[Epoch 18, Batch  1500] loss: 0.319\n",
      "[Epoch 19, Batch   100] loss: 0.194\n",
      "[Epoch 19, Batch   200] loss: 0.185\n",
      "[Epoch 19, Batch   300] loss: 0.199\n",
      "[Epoch 19, Batch   400] loss: 0.190\n",
      "[Epoch 19, Batch   500] loss: 0.166\n",
      "[Epoch 19, Batch   600] loss: 0.235\n",
      "[Epoch 19, Batch   700] loss: 0.205\n",
      "[Epoch 19, Batch   800] loss: 0.250\n",
      "[Epoch 19, Batch   900] loss: 0.245\n",
      "[Epoch 19, Batch  1000] loss: 0.283\n",
      "[Epoch 19, Batch  1100] loss: 0.262\n",
      "[Epoch 19, Batch  1200] loss: 0.256\n",
      "[Epoch 19, Batch  1300] loss: 0.234\n",
      "[Epoch 19, Batch  1400] loss: 0.287\n",
      "[Epoch 19, Batch  1500] loss: 0.308\n",
      "[Epoch 20, Batch   100] loss: 0.216\n",
      "[Epoch 20, Batch   200] loss: 0.192\n",
      "[Epoch 20, Batch   300] loss: 0.223\n",
      "[Epoch 20, Batch   400] loss: 0.243\n",
      "[Epoch 20, Batch   500] loss: 0.271\n",
      "[Epoch 20, Batch   600] loss: 0.240\n",
      "[Epoch 20, Batch   700] loss: 0.228\n",
      "[Epoch 20, Batch   800] loss: 0.218\n",
      "[Epoch 20, Batch   900] loss: 0.224\n",
      "[Epoch 20, Batch  1000] loss: 0.264\n",
      "[Epoch 20, Batch  1100] loss: 0.236\n",
      "[Epoch 20, Batch  1200] loss: 0.273\n",
      "[Epoch 20, Batch  1300] loss: 0.217\n",
      "[Epoch 20, Batch  1400] loss: 0.265\n",
      "[Epoch 20, Batch  1500] loss: 0.256\n",
      "[Epoch 21, Batch   100] loss: 0.196\n",
      "[Epoch 21, Batch   200] loss: 0.191\n",
      "[Epoch 21, Batch   300] loss: 0.209\n",
      "[Epoch 21, Batch   400] loss: 0.191\n",
      "[Epoch 21, Batch   500] loss: 0.190\n",
      "[Epoch 21, Batch   600] loss: 0.198\n",
      "[Epoch 21, Batch   700] loss: 0.198\n",
      "[Epoch 21, Batch   800] loss: 0.205\n",
      "[Epoch 21, Batch   900] loss: 0.235\n",
      "[Epoch 21, Batch  1000] loss: 0.257\n",
      "[Epoch 21, Batch  1100] loss: 0.255\n",
      "[Epoch 21, Batch  1200] loss: 0.250\n",
      "[Epoch 21, Batch  1300] loss: 0.266\n",
      "[Epoch 21, Batch  1400] loss: 0.236\n",
      "[Epoch 21, Batch  1500] loss: 0.257\n",
      "[Epoch 22, Batch   100] loss: 0.171\n",
      "[Epoch 22, Batch   200] loss: 0.176\n",
      "[Epoch 22, Batch   300] loss: 0.182\n",
      "[Epoch 22, Batch   400] loss: 0.186\n",
      "[Epoch 22, Batch   500] loss: 0.193\n",
      "[Epoch 22, Batch   600] loss: 0.226\n",
      "[Epoch 22, Batch   700] loss: 0.216\n",
      "[Epoch 22, Batch   800] loss: 0.201\n",
      "[Epoch 22, Batch   900] loss: 0.261\n",
      "[Epoch 22, Batch  1000] loss: 0.240\n",
      "[Epoch 22, Batch  1100] loss: 0.267\n",
      "[Epoch 22, Batch  1200] loss: 0.285\n",
      "[Epoch 22, Batch  1300] loss: 0.223\n",
      "[Epoch 22, Batch  1400] loss: 0.254\n",
      "[Epoch 22, Batch  1500] loss: 0.267\n",
      "[Epoch 23, Batch   100] loss: 0.187\n",
      "[Epoch 23, Batch   200] loss: 0.196\n",
      "[Epoch 23, Batch   300] loss: 0.195\n",
      "[Epoch 23, Batch   400] loss: 0.232\n",
      "[Epoch 23, Batch   500] loss: 0.185\n",
      "[Epoch 23, Batch   600] loss: 0.198\n",
      "[Epoch 23, Batch   700] loss: 0.212\n",
      "[Epoch 23, Batch   800] loss: 0.224\n",
      "[Epoch 23, Batch   900] loss: 0.198\n",
      "[Epoch 23, Batch  1000] loss: 0.206\n",
      "[Epoch 23, Batch  1100] loss: 0.242\n",
      "[Epoch 23, Batch  1200] loss: 0.277\n",
      "[Epoch 23, Batch  1300] loss: 0.259\n",
      "[Epoch 23, Batch  1400] loss: 0.314\n",
      "[Epoch 23, Batch  1500] loss: 0.267\n",
      "[Epoch 24, Batch   100] loss: 0.228\n",
      "[Epoch 24, Batch   200] loss: 0.194\n",
      "[Epoch 24, Batch   300] loss: 0.193\n",
      "[Epoch 24, Batch   400] loss: 0.193\n",
      "[Epoch 24, Batch   500] loss: 0.181\n",
      "[Epoch 24, Batch   600] loss: 0.200\n",
      "[Epoch 24, Batch   700] loss: 0.187\n",
      "[Epoch 24, Batch   800] loss: 0.211\n",
      "[Epoch 24, Batch   900] loss: 0.191\n",
      "[Epoch 24, Batch  1000] loss: 0.224\n",
      "[Epoch 24, Batch  1100] loss: 0.206\n",
      "[Epoch 24, Batch  1200] loss: 0.223\n",
      "[Epoch 24, Batch  1300] loss: 0.260\n",
      "[Epoch 24, Batch  1400] loss: 0.287\n",
      "[Epoch 24, Batch  1500] loss: 0.303\n",
      "[Epoch 25, Batch   100] loss: 0.191\n",
      "[Epoch 25, Batch   200] loss: 0.216\n",
      "[Epoch 25, Batch   300] loss: 0.184\n",
      "[Epoch 25, Batch   400] loss: 0.186\n",
      "[Epoch 25, Batch   500] loss: 0.213\n",
      "[Epoch 25, Batch   600] loss: 0.213\n",
      "[Epoch 25, Batch   700] loss: 0.276\n",
      "[Epoch 25, Batch   800] loss: 0.262\n",
      "[Epoch 25, Batch   900] loss: 0.257\n",
      "[Epoch 25, Batch  1000] loss: 0.282\n",
      "[Epoch 25, Batch  1100] loss: 0.259\n",
      "[Epoch 25, Batch  1200] loss: 0.223\n",
      "[Epoch 25, Batch  1300] loss: 0.241\n",
      "[Epoch 25, Batch  1400] loss: 0.259\n",
      "[Epoch 25, Batch  1500] loss: 0.260\n",
      "[Epoch 26, Batch   100] loss: 0.181\n",
      "[Epoch 26, Batch   200] loss: 0.173\n",
      "[Epoch 26, Batch   300] loss: 0.184\n",
      "[Epoch 26, Batch   400] loss: 0.209\n",
      "[Epoch 26, Batch   500] loss: 0.199\n",
      "[Epoch 26, Batch   600] loss: 0.223\n",
      "[Epoch 26, Batch   700] loss: 0.223\n",
      "[Epoch 26, Batch   800] loss: 0.187\n",
      "[Epoch 26, Batch   900] loss: 0.196\n",
      "[Epoch 26, Batch  1000] loss: 0.252\n",
      "[Epoch 26, Batch  1100] loss: 0.256\n",
      "[Epoch 26, Batch  1200] loss: 0.263\n",
      "[Epoch 26, Batch  1300] loss: 0.259\n",
      "[Epoch 26, Batch  1400] loss: 0.282\n",
      "[Epoch 26, Batch  1500] loss: 0.284\n",
      "[Epoch 27, Batch   100] loss: 0.179\n",
      "[Epoch 27, Batch   200] loss: 0.153\n",
      "[Epoch 27, Batch   300] loss: 0.166\n",
      "[Epoch 27, Batch   400] loss: 0.181\n",
      "[Epoch 27, Batch   500] loss: 0.176\n",
      "[Epoch 27, Batch   600] loss: 0.181\n",
      "[Epoch 27, Batch   700] loss: 0.188\n",
      "[Epoch 27, Batch   800] loss: 0.201\n",
      "[Epoch 27, Batch   900] loss: 0.216\n",
      "[Epoch 27, Batch  1000] loss: 0.228\n",
      "[Epoch 27, Batch  1100] loss: 0.256\n",
      "[Epoch 27, Batch  1200] loss: 0.266\n",
      "[Epoch 27, Batch  1300] loss: 0.243\n",
      "[Epoch 27, Batch  1400] loss: 0.267\n",
      "[Epoch 27, Batch  1500] loss: 0.231\n",
      "[Epoch 28, Batch   100] loss: 0.157\n",
      "[Epoch 28, Batch   200] loss: 0.174\n",
      "[Epoch 28, Batch   300] loss: 0.157\n",
      "[Epoch 28, Batch   400] loss: 0.183\n",
      "[Epoch 28, Batch   500] loss: 0.146\n",
      "[Epoch 28, Batch   600] loss: 0.165\n",
      "[Epoch 28, Batch   700] loss: 0.162\n",
      "[Epoch 28, Batch   800] loss: 0.203\n",
      "[Epoch 28, Batch   900] loss: 0.208\n",
      "[Epoch 28, Batch  1000] loss: 0.240\n",
      "[Epoch 28, Batch  1100] loss: 0.265\n",
      "[Epoch 28, Batch  1200] loss: 0.220\n",
      "[Epoch 28, Batch  1300] loss: 0.243\n",
      "[Epoch 28, Batch  1400] loss: 0.259\n",
      "[Epoch 28, Batch  1500] loss: 0.263\n",
      "[Epoch 29, Batch   100] loss: 0.204\n",
      "[Epoch 29, Batch   200] loss: 0.172\n",
      "[Epoch 29, Batch   300] loss: 0.158\n",
      "[Epoch 29, Batch   400] loss: 0.177\n",
      "[Epoch 29, Batch   500] loss: 0.185\n",
      "[Epoch 29, Batch   600] loss: 0.186\n",
      "[Epoch 29, Batch   700] loss: 0.206\n",
      "[Epoch 29, Batch   800] loss: 0.198\n",
      "[Epoch 29, Batch   900] loss: 0.180\n",
      "[Epoch 29, Batch  1000] loss: 0.221\n",
      "[Epoch 29, Batch  1100] loss: 0.224\n",
      "[Epoch 29, Batch  1200] loss: 0.229\n",
      "[Epoch 29, Batch  1300] loss: 0.223\n",
      "[Epoch 29, Batch  1400] loss: 0.210\n",
      "[Epoch 29, Batch  1500] loss: 0.209\n",
      "[Epoch 30, Batch   100] loss: 0.194\n",
      "[Epoch 30, Batch   200] loss: 0.216\n",
      "[Epoch 30, Batch   300] loss: 0.205\n",
      "[Epoch 30, Batch   400] loss: 0.195\n",
      "[Epoch 30, Batch   500] loss: 0.206\n",
      "[Epoch 30, Batch   600] loss: 0.175\n",
      "[Epoch 30, Batch   700] loss: 0.180\n",
      "[Epoch 30, Batch   800] loss: 0.204\n",
      "[Epoch 30, Batch   900] loss: 0.274\n",
      "[Epoch 30, Batch  1000] loss: 0.199\n",
      "[Epoch 30, Batch  1100] loss: 0.235\n",
      "[Epoch 30, Batch  1200] loss: 0.235\n",
      "[Epoch 30, Batch  1300] loss: 0.257\n",
      "[Epoch 30, Batch  1400] loss: 0.269\n",
      "[Epoch 30, Batch  1500] loss: 0.269\n",
      "Done Training!\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Training...\")\n",
    "for epoch in range(30):\n",
    "    # 我们用一个变量来记录每100个batch的平均loss\n",
    "    loss100 = 0.0\n",
    "    # 我们的dataloader派上了用场\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device) # 注意需要复制到GPU\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        # print(outputs.shape)\n",
    "        # print(labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss100 += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[Epoch %d, Batch %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, loss100 / 100))\n",
    "            loss100 = 0.0\n",
    "\n",
    "print(\"Done Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 59 %\n"
     ]
    }
   ],
   "source": [
    "# 构造测试的dataloader\n",
    "dataiter = iter(testloader)\n",
    "# 预测正确的数量和总数量\n",
    "correct = 0\n",
    "total = 0\n",
    "# 使用torch.no_grad的话在前向传播中不记录梯度，节省内存\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # 预测\n",
    "        outputs = net(images)\n",
    "        # 我们的网络输出的实际上是个概率分布，去最大概率的哪一项作为预测分类\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
